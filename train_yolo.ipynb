{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35d940d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m714s\u001b[0m 2s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.6526 - damage_type_accuracy: 0.3491 - damage_type_loss: 1.3748 - loss: 2.0274\n",
      "Epoch 2/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m613s\u001b[0m 2s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.5111 - damage_type_accuracy: 0.4437 - damage_type_loss: 1.3254 - loss: 1.8364\n",
      "Epoch 3/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m837s\u001b[0m 2s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.4032 - damage_type_accuracy: 0.3995 - damage_type_loss: 1.3052 - loss: 1.7084\n",
      "Epoch 4/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4648s\u001b[0m 14s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.3211 - damage_type_accuracy: 0.4678 - damage_type_loss: 1.2746 - loss: 1.5957\n",
      "Epoch 5/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m734s\u001b[0m 2s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.2583 - damage_type_accuracy: 0.4298 - damage_type_loss: 1.2718 - loss: 1.5301\n",
      "Epoch 6/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m639s\u001b[0m 2s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.2099 - damage_type_accuracy: 0.4055 - damage_type_loss: 1.2607 - loss: 1.4705\n",
      "Epoch 7/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15468s\u001b[0m 45s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.1721 - damage_type_accuracy: 0.4641 - damage_type_loss: 1.2249 - loss: 1.3970\n",
      "Epoch 8/8\n",
      "\u001b[1m342/342\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m754s\u001b[0m 2s/step - anomaly_detected_accuracy: 1.0000 - anomaly_detected_loss: 0.1422 - damage_type_accuracy: 0.4349 - damage_type_loss: 1.2283 - loss: 1.3705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Modèle sauvegardé\n",
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpwi7dvqlo\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\user\\AppData\\Local\\Temp\\tmpwi7dvqlo\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'C:\\Users\\user\\AppData\\Local\\Temp\\tmpwi7dvqlo'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): List[TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_T0'), TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='input_T1')]\n",
      "Output Type:\n",
      "  Dict[['anomaly_detected', TensorSpec(shape=(None, 1), dtype=tf.float32, name=None)], ['damage_type', TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)]]\n",
      "Captures:\n",
      "  2457242639760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242640144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242640336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242641296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242641488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242642064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242642256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242642832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242643024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242643600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242643792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242644368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242645328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242645904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242644560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  2457242645136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "✅ Export TFLite terminé !\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.utils import Sequence, to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "\n",
    "DAMAGE_TYPES = ['front', 'back', 'left', 'right']\n",
    "\n",
    "class DamageDataGenerator(Sequence):\n",
    "    def __init__(self, base_path, batch_size=8, input_shape=(224,224,3)):\n",
    "        self.base_path = base_path\n",
    "        self.batch_size = batch_size\n",
    "        self.input_shape = input_shape\n",
    "        self.T0_path = os.path.join(base_path, \"T0\")\n",
    "        self.T1_path = os.path.join(base_path, \"T1\")\n",
    "        self.damage_types = DAMAGE_TYPES\n",
    "        self.file_paths = []\n",
    "        for damage_type in self.damage_types:\n",
    "            T0_files = os.listdir(os.path.join(self.T0_path, damage_type))\n",
    "            T1_files = os.listdir(os.path.join(self.T1_path, damage_type))\n",
    "            for file in T0_files:\n",
    "                if file in T1_files:\n",
    "                    self.file_paths.append((os.path.join(self.T0_path, damage_type, file), os.path.join(self.T1_path, damage_type, file)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.file_paths) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        batch_files = self.file_paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "        batch_T0 = np.zeros((len(batch_files), *self.input_shape), dtype=np.float32)\n",
    "        batch_T1 = np.zeros((len(batch_files), *self.input_shape), dtype=np.float32)\n",
    "        batch_labels = np.zeros((len(batch_files),), dtype=np.int32)\n",
    "        for i, (T0_file, T1_file) in enumerate(batch_files):\n",
    "            T0_img = load_img(T0_file, target_size=self.input_shape[:2])\n",
    "            T1_img = load_img(T1_file, target_size=self.input_shape[:2])\n",
    "            batch_T0[i] = img_to_array(T0_img) / 255.0\n",
    "            batch_T1[i] = img_to_array(T1_img) / 255.0\n",
    "            damage_type = os.path.basename(os.path.dirname(T0_file))\n",
    "            batch_labels[i] = self.damage_types.index(damage_type)\n",
    "        batch_anomaly = (np.abs(np.mean(batch_T1, axis=(1,2,3)) - np.mean(batch_T0, axis=(1,2,3))) > 0.05).astype(np.float32)\n",
    "        batch_damage_type_onehot = to_categorical(batch_labels, num_classes=len(self.damage_types))\n",
    "        return {\"input_T0\": batch_T0, \"input_T1\": batch_T1}, {\"anomaly_detected\": batch_anomaly, \"damage_type\": batch_damage_type_onehot}\n",
    "\n",
    "def create_dual_input_model(input_shape=(224,224,3), num_damage_types=4):\n",
    "    def branch():\n",
    "        inp = Input(shape=input_shape)\n",
    "        x = layers.Conv2D(16, (3,3), activation='relu')(inp)\n",
    "        x = layers.MaxPooling2D((2,2))(x)\n",
    "        x = layers.Conv2D(32, (3,3), activation='relu')(x)\n",
    "        x = layers.MaxPooling2D((2,2))(x)\n",
    "        x = layers.Conv2D(64, (3,3), activation='relu')(x)\n",
    "        x = layers.MaxPooling2D((2,2))(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        return models.Model(inp, x)\n",
    "\n",
    "    input_T0 = Input(shape=input_shape, name=\"input_T0\")\n",
    "    input_T1 = Input(shape=input_shape, name=\"input_T1\")\n",
    "\n",
    "    extractor = branch()\n",
    "    feat_T0 = extractor(input_T0)\n",
    "    feat_T1 = extractor(input_T1)\n",
    "\n",
    "    x = layers.Subtract()([feat_T1, feat_T0])\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "\n",
    "    anomaly_output = layers.Dense(1, activation='sigmoid', name=\"anomaly_detected\")(x)\n",
    "    damage_output = layers.Dense(num_damage_types, activation='softmax', name=\"damage_type\")(x)\n",
    "\n",
    "    return models.Model(inputs={\"input_T0\": input_T0, \"input_T1\": input_T1}, outputs={\"anomaly_detected\": anomaly_output, \"damage_type\": damage_output})\n",
    "\n",
    "def main():\n",
    "    base_path = \"dataset\"  \n",
    "    batch_size = 8\n",
    "    input_shape = (224, 224, 3)\n",
    "    epochs = 8\n",
    "\n",
    "    train_generator = DamageDataGenerator(base_path, batch_size=batch_size, input_shape=input_shape)\n",
    "\n",
    "    model = create_dual_input_model(input_shape=input_shape, num_damage_types=len(DAMAGE_TYPES))\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={\n",
    "            \"anomaly_detected\": \"binary_crossentropy\",\n",
    "            \"damage_type\": \"categorical_crossentropy\"\n",
    "        },\n",
    "        metrics={\n",
    "            \"anomaly_detected\": \"accuracy\",\n",
    "            \"damage_type\": \"accuracy\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    model.fit(train_generator, epochs=epochs)\n",
    "\n",
    "    model.save(\"damage_comparison_model.h5\")\n",
    "    print(\"✅ Modèle sauvegardé\")\n",
    "\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    tflite_model = converter.convert()\n",
    "    with open(\"damage_comparison_model.tflite\", \"wb\") as f:\n",
    "        f.write(tflite_model)\n",
    "    print(\"✅ Export TFLite terminé !\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
